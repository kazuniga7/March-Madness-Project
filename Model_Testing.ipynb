{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uoYWJ7UjXi0e"
      },
      "outputs": [],
      "source": [
        "# Model testing Code (All code below this comment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ko1nI5ZTzHez"
      },
      "outputs": [],
      "source": [
        "# Kevin Code\n",
        "# Importing March Madness data we scraped\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df_combined_22_23 = pd.read_csv(\"MarchMadness2022-23.csv\")\n",
        "df_combined_23_24 = pd.read_csv(\"MarchMadness2023-24.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "novGuLdxAuo2"
      },
      "outputs": [],
      "source": [
        "# Mack Code\n",
        "# Model libraries needed along with scalers and metrics\n",
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.preprocessing import (MaxAbsScaler, MinMaxScaler, Normalizer, PowerTransformer, QuantileTransformer, RobustScaler, StandardScaler)\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "scalers = [MaxAbsScaler(), MinMaxScaler(), Normalizer(), PowerTransformer(), QuantileTransformer(n_quantiles=60, output_distribution='normal'), RobustScaler(), StandardScaler()]\n",
        "dist_metrics = [\"euclidean\", \"manhattan\", \"chebyshev\", \"minkowski\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITAzXBAARO3H"
      },
      "outputs": [],
      "source": [
        "# Mack code\n",
        "# model 1 -- logistic regression --> hyperparameters then features\n",
        "\n",
        "# opinion of best features\n",
        "features = [\"Seed_diff\", \"W-L%_diff\", \"SRS_diff\", \"SOS_diff\", \"Tm._diff\", \"Opp._diff\", \"FG%_diff\", \"3P%_diff\", \"TRB_diff\", \"AST_diff\", \"STL_diff\", \"BLK_diff\", \"TOV_diff\", \"PF_diff\"]\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "X_train = df_combined_22_23[features]\n",
        "Y_train = df_combined_22_23[\"winner\"]\n",
        "\n",
        "models = {}\n",
        "\n",
        "for scaler in scalers:\n",
        "  pipeline = make_pipeline(scaler, LogisticRegression())\n",
        "\n",
        "  grid_cv = GridSearchCV(pipeline,\n",
        "                         param_grid = {\n",
        "                        \"logisticregression__C\": [0.01, 0.1, 1, 10],\n",
        "                        \"logisticregression__solver\": [\"lbfgs\", \"liblinear\"],\n",
        "                        \"logisticregression__penalty\": [\"l2\"]\n",
        "},\n",
        "                         scoring=\"f1_macro\", cv=10)\n",
        "\n",
        "  grid_cv.fit(X_train, Y_train)\n",
        "\n",
        "  potential_model = [f\"Scaler: {scaler}\"]\n",
        "  for key, value in (grid_cv.best_params_).items():\n",
        "    potential_model.append(f\"{key}: {value}\")\n",
        "  potential_model = tuple(potential_model)\n",
        "  models[potential_model] = grid_cv.best_score_\n",
        "  print(scaler)\n",
        "\n",
        "models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANgT_4jjAzCN"
      },
      "outputs": [],
      "source": [
        "# Mack code\n",
        "# Finding the best hyperparameters\n",
        "\n",
        "best_hypes = ()\n",
        "score = 0\n",
        "for key in models.keys():\n",
        "  if models[key] > score:\n",
        "    score = models[key]\n",
        "    best_hypes = key\n",
        "\n",
        "best_hypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCPphsqsA1AE"
      },
      "outputs": [],
      "source": [
        "# Mack code\n",
        "best_scaler = RobustScaler()\n",
        "best_c = 0.01\n",
        "best_penalty = \"l2\"\n",
        "best_solver = \"lbfgs\"\n",
        "\n",
        "# Choosing best input features\n",
        "\n",
        "from itertools import combinations\n",
        "\n",
        "best_pipeline = make_pipeline(best_scaler, LogisticRegression(C=best_c, solver=best_solver, penalty=best_penalty))\n",
        "\n",
        "best_score = 0\n",
        "best_features = []\n",
        "for i in range(1,len(features) + 1):\n",
        "  for combo in combinations(features, i):\n",
        "    features_combo = list(combo)\n",
        "    score = cross_val_score(best_pipeline,\n",
        "                    X=df_combined_22_23[features_combo], y=df_combined_22_23[\"winner\"],\n",
        "                    scoring=\"f1_macro\", cv=10).mean()\n",
        "    if score > best_score:\n",
        "      best_score = score\n",
        "      best_features = features_combo\n",
        "\n",
        "\n",
        "print(f\"{best_features}: {best_score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1a_5JybkA2gR"
      },
      "outputs": [],
      "source": [
        "# Mack code\n",
        "best_features_logistic = [\"SOS_diff\", \"Tm._diff\", \"Opp._diff\", \"FG%_diff\", \"3P%_diff\", \"TRB_diff\"]\n",
        "\n",
        "best_pipeline_logistic = make_pipeline(RobustScaler(), LogisticRegression(C=0.01, solver=\"lbfgs\", penalty=\"l2\"))\n",
        "\n",
        "x_train = df_combined_22_23[best_features_logistic]\n",
        "y_train = df_combined_22_23[\"winner\"]\n",
        "\n",
        "x_test = df_combined_23_24[best_features_logistic]\n",
        "y_test = df_combined_23_24[\"winner\"]\n",
        "\n",
        "best_pipeline_logistic.fit(x_train, y_train)\n",
        "\n",
        "best_pipeline_logistic.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19beMKLWmAaY"
      },
      "outputs": [],
      "source": [
        "# Mack code\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Estimate the test error for the Logistic Regression model using 10-fold cross-validation on the 23-24 data\n",
        "best_logistic_YTest_f1macro = cross_val_score(\n",
        "    best_pipeline_logistic,\n",
        "    x_test,\n",
        "    y_test,\n",
        "    scoring=\"f1_macro\",\n",
        "    cv=10\n",
        ").mean()\n",
        "\n",
        "print(f\"Best Logistic Regression f1_score: {best_logistic_YTest_f1macro}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-8nJXYDMpLO"
      },
      "outputs": [],
      "source": [
        "# Mack code\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
        "\n",
        "best_features_logistic = [\"SOS_diff\", \"Tm._diff\", \"Opp._diff\", \"FG%_diff\", \"3P%_diff\", \"TRB_diff\"]\n",
        "\n",
        "best_pipeline_logistic = make_pipeline(\n",
        "    RobustScaler(),\n",
        "    LogisticRegression(C=0.01, solver=\"lbfgs\", penalty=\"l2\", random_state=42)\n",
        ")\n",
        "\n",
        "# Training data: 2022-23\n",
        "X_train = df_combined_22_23[best_features_logistic]\n",
        "y_train = df_combined_22_23[\"winner\"]\n",
        "\n",
        "# Test data: 2023-24\n",
        "X_test = df_combined_23_24[best_features_logistic]\n",
        "y_test = df_combined_23_24[\"winner\"]\n",
        "\n",
        "# Fit the pipeline on 22-23 data\n",
        "best_pipeline_logistic.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the 23-24 data\n",
        "y_pred = best_pipeline_logistic.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy on 2023-24 data: {accuracy:.3f}\")\n",
        "print(f\"F1-macro on 2023-24 data: {f1:.3f}\")\n",
        "print(\"Confusion Matrix:\\n\", cm)\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7z1iNeyS9Wp"
      },
      "source": [
        ".83"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GymzfsYpRtU3"
      },
      "outputs": [],
      "source": [
        "# Mack code\n",
        "y_proba = best_pipeline_logistic.predict_proba(X_test)[:, 1]\n",
        "\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 2. Compute precision and recall at various thresholds\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_proba, pos_label=1)\n",
        "\n",
        "# 3. Plot the precision-recall curve\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(recall, precision, label=\"Precision-Recall Curve\")\n",
        "plt.xlabel(\"Recall\", fontsize=14)\n",
        "plt.ylabel(\"Precision\", fontsize=14)\n",
        "plt.title(\"Figure 1 - LogisticRegression Precision-Recall Curve\", fontsize=14)\n",
        "plt.savefig(\"logistic_precision_recall.png\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRuA3_w5BIg1"
      },
      "outputs": [],
      "source": [
        "# Mack code\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Get the trained LogisticRegression model from your pipeline\n",
        "logistic_model = best_pipeline_logistic.named_steps[\"logisticregression\"]\n",
        "\n",
        "# 2. Extract the coefficients (coef_ is a 2D array, with shape [n_classes, n_features])\n",
        "#    For a binary classification with one set of coefficients, use logistic_model.coef_[0]\n",
        "coefs = logistic_model.coef_[0]\n",
        "\n",
        "# 3. Create a DataFrame for easy sorting and plotting\n",
        "feature_names = x_train.columns\n",
        "feat_imp_df = pd.DataFrame({\"Feature\": feature_names, \"Coefficient\": coefs})\n",
        "\n",
        "# 4. Sort by absolute coefficient value (largest magnitude is most influential)\n",
        "feat_imp_df[\"abs_coeff\"] = feat_imp_df[\"Coefficient\"].abs()\n",
        "feat_imp_df.sort_values(\"abs_coeff\", ascending=False, inplace=True)\n",
        "\n",
        "# 5. Plot\n",
        "plt.barh(feat_imp_df[\"Feature\"], feat_imp_df[\"Coefficient\"])\n",
        "plt.xlabel(\"Coefficient Value\")\n",
        "plt.title(\"Figure 2 - Logistic Regression Coefficients\")\n",
        "plt.gca().invert_yaxis()  # Top features on top\n",
        "plt.savefig(\"logistic_coefficients.png\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6K08Zqg0C9_8"
      },
      "outputs": [],
      "source": [
        "# Mack code\n",
        "# model 1 -- logisticregression (input features --> Hyperparameters)\n",
        "\n",
        "# DO NOT RUN THIS CELL\n",
        "\n",
        "from itertools import combinations\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "start_pipeline = make_pipeline(StandardScaler(), LogisticRegression())\n",
        "\n",
        "best_score = 0\n",
        "best_features = []\n",
        "for i in tqdm(range(1,len(features) + 1)):\n",
        "  for combo in tqdm(combinations(features, i)):\n",
        "    features_combo = list(combo)\n",
        "    score = cross_val_score(start_pipeline,\n",
        "                    X=df_combined_22_23[features_combo], y=df_combined_22_23[\"winner\"],\n",
        "                    scoring=\"f1_macro\", cv=10).mean()\n",
        "    if score > best_score:\n",
        "      best_score = score\n",
        "      best_features = features_combo\n",
        "print(f\"{best_features}: {best_score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMtKSOvqSuvp"
      },
      "source": [
        "0.8063492063492064"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sb8fIVv-E2U2"
      },
      "outputs": [],
      "source": [
        "# Mack code\n",
        "# DO NOT RUN THIS CELL\n",
        "\n",
        "X_train = df_combined_22_23[best_features]\n",
        "Y_train = df_combined_22_23[\"winner\"]\n",
        "\n",
        "models = {}\n",
        "\n",
        "for scaler in tqdm(scalers):\n",
        "  pipeline = make_pipeline(scaler, LogisticRegression())\n",
        "\n",
        "  grid_cv = GridSearchCV(pipeline,\n",
        "                         param_grid={},\n",
        "                         scoring=\"f1_macro\", cv=10)\n",
        "  grid_cv.fit(X_train, Y_train)\n",
        "\n",
        "  potential_model = [f\"Scaler: {scaler}\"]\n",
        "  for key, value in (grid_cv.best_params_).items():\n",
        "    potential_model.append(f\"{key}: {value}\")\n",
        "  potential_model = tuple(potential_model)\n",
        "  models[potential_model] = grid_cv.best_score_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10HsgF71E201"
      },
      "outputs": [],
      "source": [
        "# Mack code\n",
        "# DO NOT RUN THIS CELL\n",
        "\n",
        "best_hypes = ()\n",
        "best_score = 0\n",
        "for key in models.keys():\n",
        "  if models[key] > best_score:\n",
        "    best_score = models[key]\n",
        "    best_hypes = key\n",
        "\n",
        "best_hypes\n",
        "print(f\"{best_hypes}: {best_score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVu8RMCTDfbi"
      },
      "outputs": [],
      "source": [
        "# Mack code\n",
        "# RUN THIS ONE!!\n",
        "\n",
        "logistic_HypInputs_Scaler = RobustScaler()\n",
        "logistic_HypInputs_c = 0.01\n",
        "logistic_HypInputs_penalty = \"l2\"\n",
        "logistic_HypInputs_solver = \"lbfgs\"\n",
        "\n",
        "\n",
        "logistic_HypInputs_features = ['SRS_diff', 'Opp._diff', 'PF_diff']\n",
        "logistic_HypInputs_Pipeline = make_pipeline(logistic_HypInputs_Scaler,\n",
        "                                                   LogisticRegression(C=logistic_HypInputs_c, penalty=logistic_HypInputs_penalty, solver=logistic_HypInputs_solver))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOnkPUuzNfvF"
      },
      "outputs": [],
      "source": [
        "# Mack code\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
        "\n",
        "# Features, pipeline, and hyperparams as given\n",
        "logistic_HypInputs_Scaler = RobustScaler()\n",
        "logistic_HypInputs_c = 0.01\n",
        "logistic_HypInputs_penalty = \"l2\"\n",
        "logistic_HypInputs_solver = \"lbfgs\"\n",
        "\n",
        "logistic_HypInputs_features = ['SRS_diff', 'Opp._diff', 'PF_diff']\n",
        "\n",
        "logistic_HypInputs_Pipeline = make_pipeline(\n",
        "    logistic_HypInputs_Scaler,\n",
        "    LogisticRegression(\n",
        "        C=logistic_HypInputs_c,\n",
        "        penalty=logistic_HypInputs_penalty,\n",
        "        solver=logistic_HypInputs_solver,\n",
        "        random_state=42\n",
        "    )\n",
        ")\n",
        "\n",
        "# 1. Separate training (22–23) and testing (23–24) sets\n",
        "X_train = df_combined_22_23[logistic_HypInputs_features]\n",
        "y_train = df_combined_22_23[\"winner\"]\n",
        "\n",
        "X_test = df_combined_23_24[logistic_HypInputs_features]\n",
        "y_test = df_combined_23_24[\"winner\"]\n",
        "\n",
        "# 2. Fit the pipeline on 22–23 data\n",
        "logistic_HypInputs_Pipeline.fit(X_train, y_train)\n",
        "\n",
        "# 3. Predict on 23–24 data\n",
        "y_pred = logistic_HypInputs_Pipeline.predict(X_test)\n",
        "\n",
        "# 4. Evaluate\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy on 2023–24 data: {accuracy:.3f}\")\n",
        "print(f\"F1-macro on 2023–24 data: {f1:.3f}\")\n",
        "print(\"Confusion Matrix:\\n\", cm)\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQI4xL9xR5bz"
      },
      "outputs": [],
      "source": [
        "# Mack code\n",
        "y_proba = logistic_HypInputs_Pipeline.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# 2. Compute precision and recall at various thresholds\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_proba, pos_label=1)\n",
        "\n",
        "# 3. Plot the precision-recall curve\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(recall, precision, label=\"Precision-Recall Curve\")\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"Precision-Recall Curve (2023-24 Data)\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGKw9kxlTzZa"
      },
      "outputs": [],
      "source": [
        "# Mack code\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Get the trained LogisticRegression model from your pipeline\n",
        "logistic_model = logistic_HypInputs_Pipeline.named_steps[\"logisticregression\"]\n",
        "\n",
        "# Ensure the pipeline is fit before accessing coefficients:\n",
        "logistic_HypInputs_Pipeline.fit(df_combined_22_23[logistic_HypInputs_features], df_combined_22_23[\"winner\"]) # This line was added\n",
        "\n",
        "# 2. Extract the coefficients (coef_ is a 2D array, with shape [n_classes, n_features])\n",
        "#    For a binary classification with one set of coefficients, use logistic_model.coef_[0]\n",
        "coefs = logistic_model.coef_[0]\n",
        "\n",
        "# 3. Create a DataFrame for easy sorting and plotting\n",
        "feat_imp_df = pd.DataFrame({\"Feature\": logistic_HypInputs_features, \"Coefficient\": coefs})\n",
        "\n",
        "# 4. Sort by absolute coefficient value (largest magnitude is most influential)\n",
        "feat_imp_df[\"abs_coeff\"] = feat_imp_df[\"Coefficient\"].abs()\n",
        "feat_imp_df.sort_values(\"abs_coeff\", ascending=False, inplace=True)\n",
        "\n",
        "# 5. Plot\n",
        "plt.barh(feat_imp_df[\"Feature\"], feat_imp_df[\"Coefficient\"])\n",
        "plt.xlabel(\"Coefficient Value\\n(Positive => Increases Probability, Negative => Decreases Probability)\")\n",
        "plt.title(\"Logistic Regression Coefficients\")\n",
        "plt.gca().invert_yaxis()  # Top features on top\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPsRnB7UYatS"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KISJxuaCypy"
      },
      "outputs": [],
      "source": [
        "# Mack code\n",
        "# ensemble of logisticregression\n",
        "\n",
        "# ensemble model of randomforestclassifier\n",
        "\n",
        "# Create two pipelines with different scalers\n",
        "pipeline1 = make_pipeline(RobustScaler(), LogisticRegression(C=0.01, solver=\"lbfgs\", penalty=\"l2\"))\n",
        "\n",
        "pipeline2 = make_pipeline(logistic_HypInputs_Scaler,\n",
        "                                                   LogisticRegression(C=logistic_HypInputs_c, penalty=logistic_HypInputs_penalty, solver=logistic_HypInputs_solver))\n",
        "\n",
        "# Create a VotingClassifier that combines the pipelines (using soft voting)\n",
        "ensemble_model = VotingClassifier(\n",
        "    estimators=[('standard_rf', pipeline1), ('minmax_rf', pipeline2)],\n",
        "    voting='soft'\n",
        ")\n",
        "\n",
        "# Evaluate using cross-validation\n",
        "scores = cross_val_score(ensemble_model, x_train, y_train, cv=10, scoring=\"f1_macro\", n_jobs=-1)\n",
        "print(\"Cross-validated F1-macro scores:\", scores)\n",
        "print(\"Mean F1-macro score:\", np.mean(scores))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbs3hEGBDaJc"
      },
      "source": [
        ".7463888"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "CsJ8bZiRsBh_"
      },
      "outputs": [],
      "source": [
        "# Mack code\n",
        "# model 2 -- randomforest --> hyperparameters then input features\n",
        "\n",
        "# DONT NEED TO RUN CELL --> CORRECT OUTPUT BELOW\n",
        "\n",
        "features = [\"Seed_diff\", \"W-L%_diff\", \"SRS_diff\", \"SOS_diff\", \"Tm._diff\", \"Opp._diff\", \"FG%_diff\",\n",
        "            \"3P%_diff\", \"TRB_diff\", \"AST_diff\", \"STL_diff\", \"BLK_diff\", \"TOV_diff\", \"PF_diff\"]\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "X_train = df_combined_22_23[features]\n",
        "Y_train = df_combined_22_23[\"winner\"]\n",
        "\n",
        "models = {}\n",
        "\n",
        "for scaler in scalers:\n",
        "    pipeline = make_pipeline(scaler, RandomForestClassifier(random_state=42))\n",
        "\n",
        "    grid_cv = GridSearchCV(\n",
        "        pipeline,\n",
        "        param_grid={\n",
        "            \"randomforestclassifier__n_estimators\": [50, 100, 200],\n",
        "            \"randomforestclassifier__max_depth\": [None, 5, 10, 20],\n",
        "            \"randomforestclassifier__min_samples_split\": [2, 5, 10]\n",
        "        },\n",
        "        scoring=\"f1_macro\",\n",
        "        cv=10\n",
        "    )\n",
        "\n",
        "    grid_cv.fit(X_train, Y_train)\n",
        "\n",
        "    potential_model = [f\"Scaler: {scaler}\"]\n",
        "    for key, value in grid_cv.best_params_.items():\n",
        "        potential_model.append(f\"{key}: {value}\")\n",
        "    potential_model = tuple(potential_model)\n",
        "    models[potential_model] = grid_cv.best_score_\n",
        "    print(scaler)\n",
        "\n",
        "models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "yKyQeigZBXIv"
      },
      "outputs": [],
      "source": [
        "# Mack code\n",
        "# Finding the best hyperparameters\n",
        "\n",
        "# DO NOT NEED TO RUN CELL\n",
        "\n",
        "best_hypes = ()\n",
        "score = 0\n",
        "for key in models.keys():\n",
        "  if models[key] > score:\n",
        "    score = models[key]\n",
        "    best_hypes = key\n",
        "\n",
        "best_hypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "VTB8szktBYpU"
      },
      "outputs": [],
      "source": [
        "# Mack code\n",
        "# DO NOT NEED TO RUN CELL\n",
        "\n",
        "best_scaler = MaxAbsScaler()\n",
        "best_depth = 5\n",
        "best_split = 2\n",
        "best_estimators = 100\n",
        "\n",
        "# Choosing best input features\n",
        "\n",
        "from itertools import combinations\n",
        "\n",
        "best_pipeline = make_pipeline(best_scaler, RandomForestClassifier(max_depth=best_depth, min_samples_split=best_split, n_estimators=best_estimators))\n",
        "\n",
        "best_score = 0\n",
        "best_features = []\n",
        "for i in range(1,len(features) + 1):\n",
        "  for combo in combinations(features, i):\n",
        "    features_combo = list(combo)\n",
        "    score = cross_val_score(best_pipeline,\n",
        "                    X=df_combined_22_23[features_combo], y=df_combined_22_23[\"winner\"],\n",
        "                    scoring=\"f1_macro\", cv=10).mean()\n",
        "    if score > best_score:\n",
        "      best_score = score\n",
        "      best_features = features_combo\n",
        "\n",
        "print(f\"{best_features}: {best_score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "De2p6s73BZgT"
      },
      "outputs": [],
      "source": [
        "# Mack code\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "best_features_forests = ['SRS_diff', 'SOS_diff', 'Tm._diff', 'Opp._diff']\n",
        "\n",
        "best_pipeline_forests = make_pipeline(MaxAbsScaler(), RandomForestClassifier(max_depth=5, min_samples_split=2, n_estimators=100))\n",
        "\n",
        "x_train = df_combined_22_23[best_features_forests]\n",
        "y_train = df_combined_22_23[\"winner\"]\n",
        "\n",
        "x_test = df_combined_23_24[best_features_forests]\n",
        "y_test = df_combined_23_24[\"winner\"]\n",
        "\n",
        "best_pipeline_forests.fit(x_train, y_train)\n",
        "\n",
        "best_pipeline_forests.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HrG7t7kjNwWR"
      },
      "outputs": [],
      "source": [
        "# Mack code\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
        "\n",
        "# Features and pipeline definition\n",
        "best_features_forests = ['SRS_diff', 'SOS_diff', 'Tm._diff', 'Opp._diff']\n",
        "best_pipeline_forests = make_pipeline(\n",
        "    MaxAbsScaler(),\n",
        "    RandomForestClassifier(max_depth=5, min_samples_split=2, n_estimators=100, random_state=42)\n",
        ")\n",
        "\n",
        "# Separate training (22–23) and testing (23–24) sets\n",
        "X_train = df_combined_22_23[best_features_forests]\n",
        "y_train = df_combined_22_23[\"winner\"]\n",
        "\n",
        "X_test = df_combined_23_24[best_features_forests]\n",
        "y_test = df_combined_23_24[\"winner\"]\n",
        "\n",
        "# Fit the pipeline on 22–23 data\n",
        "best_pipeline_forests.fit(X_train, y_train)\n",
        "\n",
        "# Predict on 23–24 data\n",
        "y_pred = best_pipeline_forests.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy on 2023–24 data: {accuracy:.3f}\")\n",
        "print(f\"F1-macro on 2023–24 data: {f1:.3f}\")\n",
        "print(\"Confusion Matrix:\\n\", cm)\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jpiomb2zSb1R"
      },
      "outputs": [],
      "source": [
        "# Mack code\n",
        "y_proba = best_pipeline_forests.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute precision, recall, and thresholds\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_proba, pos_label=1)\n",
        "\n",
        "# Plot the precision-recall curve\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(recall, precision, label=\"Precision-Recall Curve\", color='b')\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"Precision-Recall Curve (2023–24 Data)\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXhI1x98BZtl"
      },
      "outputs": [],
      "source": [
        "# Mack code\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "rf_model = best_pipeline_forests.named_steps[\"randomforestclassifier\"]\n",
        "importances = rf_model.feature_importances_\n",
        "\n",
        "# Use the features from your best model (best_features_forests)\n",
        "feature_names = best_features_forests  # Changed this line\n",
        "\n",
        "feat_imp_df = pd.DataFrame({\"Feature\": feature_names, \"Importance\": importances})\n",
        "feat_imp_df.sort_values(\"Importance\", ascending=False, inplace=True)\n",
        "\n",
        "plt.barh(feat_imp_df[\"Feature\"], feat_imp_df[\"Importance\"])\n",
        "plt.xlabel(\"Feature Importance\")\n",
        "plt.title(\"RandomForest Coefficients\")\n",
        "plt.gca().invert_yaxis()  # so top feature is at the top\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZO_OpHqBTGrE"
      },
      "outputs": [],
      "source": [
        "# Mack code\n",
        "import itertools\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Ensure reproducibility:\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "best_score = 0\n",
        "best_features = None\n",
        "\n",
        "# Optional: count total possible subsets (excluding empty set)\n",
        "total_subsets = sum(1 for i in range(1, len(features)+1) for _ in itertools.combinations(features, i))\n",
        "print(\"Total subsets:\", total_subsets)\n",
        "\n",
        "count_tested = 0\n",
        "\n",
        "# Iterate over all subset sizes\n",
        "for i in range(1, 8):\n",
        "    # List all combinations of size i (wrap with list() to use tqdm)\n",
        "    for combo in tqdm(list(itertools.combinations(features, i)), desc=f\"Testing combinations of size {i}\"):\n",
        "        # Randomly decide to test this combination with a probability of 0.5\n",
        "        if random.random() < 0.5:\n",
        "            features_combo = list(combo)\n",
        "            # Evaluate this combination using cross-validation\n",
        "            cv_score = cross_val_score(\n",
        "                make_pipeline(StandardScaler(), RandomForestClassifier(n_jobs=-1, random_state=42)),\n",
        "                X=df_combined_22_23[features_combo],\n",
        "                y=df_combined_22_23[\"winner\"],\n",
        "                scoring=\"f1_macro\",\n",
        "                cv=5,\n",
        "                n_jobs=-1\n",
        "            ).mean()\n",
        "            count_tested += 1\n",
        "            if cv_score > best_score:\n",
        "                best_score = cv_score\n",
        "                best_features = features_combo\n",
        "\n",
        "print(f\"Tested {count_tested} combinations out of {total_subsets} total subsets.\")\n",
        "print(f\"Best combination: {best_features} with F1-macro score = {best_score:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnbUJYJ8-LeQ"
      },
      "source": [
        "Tested 8159 combinations out of 16383 total subsets.\n",
        "Best combination: ['Seed_diff', 'SOS_diff', 'Opp._diff', 'TRB_diff', 'STL_diff', 'BLK_diff', 'PF_diff']\n",
        "with F1-macro score = 0.804"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yezovssTZma"
      },
      "outputs": [],
      "source": [
        "# Mack code\n",
        "# DO NOT RUN THIS CELL\n",
        "\n",
        "forests_2_best_features = ['Seed_diff', 'SOS_diff', 'Opp._diff', 'TRB_diff', 'STL_diff', 'BLK_diff', 'PF_diff']\n",
        "\n",
        "X_train = df_combined_22_23[forests_2_best_features]\n",
        "Y_train = df_combined_22_23[\"winner\"]\n",
        "\n",
        "models = {}\n",
        "\n",
        "for scaler in tqdm(scalers):\n",
        "  pipeline = make_pipeline(scaler, RandomForestClassifier())\n",
        "\n",
        "  grid_cv = GridSearchCV(pipeline,\n",
        "                         param_grid={\n",
        "                        \"randomforestclassifier__n_estimators\": [50, 100, 200],\n",
        "                        \"randomforestclassifier__max_depth\": [None, 5, 10, 20],\n",
        "                        \"randomforestclassifier__min_samples_split\": [2, 5, 10]\n",
        "                    },\n",
        "                         scoring=\"f1_macro\", cv=10)\n",
        "  grid_cv.fit(X_train, Y_train)\n",
        "\n",
        "  potential_model = [f\"Scaler: {scaler}\"]\n",
        "  for key, value in (grid_cv.best_params_).items():\n",
        "    potential_model.append(f\"{key}: {value}\")\n",
        "  potential_model = tuple(potential_model)\n",
        "  models[potential_model] = grid_cv.best_score_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FEuWXtyTc0o"
      },
      "outputs": [],
      "source": [
        "# Mack code\n",
        "# DO NOT RUN THIS CELL\n",
        "\n",
        "best_hypes = ()\n",
        "best_score = 0\n",
        "for key in models.keys():\n",
        "  if models[key] > best_score:\n",
        "    best_score = models[key]\n",
        "    best_hypes = key\n",
        "\n",
        "best_hypes\n",
        "print(f\"{best_hypes}: {best_score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BG7vWY7GTdD2"
      },
      "outputs": [],
      "source": [
        "# Mack code\n",
        "# RUN THIS ONE!!\n",
        "\n",
        "forests_2_best_features = ['Seed_diff', 'SOS_diff', 'Opp._diff', 'TRB_diff', 'STL_diff', 'BLK_diff', 'PF_diff']\n",
        "\n",
        "X_train = df_combined_22_23[forests_2_best_features]\n",
        "Y_train = df_combined_22_23[\"winner\"]\n",
        "\n",
        "forests_HypInputs_Scaler = Normalizer()\n",
        "forests_HypInputs_max_depth = 5\n",
        "forests_HypInputs_min_samples_split = 5\n",
        "forests_HypInputs_n_estimators = 100\n",
        "\n",
        "forests_HypInputs_features = ['Seed_diff', 'SOS_diff', 'Opp._diff', 'TRB_diff', 'STL_diff', 'BLK_diff', 'PF_diff']\n",
        "forests_HypInputs_Pipeline = make_pipeline(forests_HypInputs_Scaler,\n",
        "                                                   RandomForestClassifier(max_depth=forests_HypInputs_max_depth, min_samples_split=forests_HypInputs_min_samples_split, n_estimators=forests_HypInputs_n_estimators))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1GBzUOamTdUC"
      },
      "outputs": [],
      "source": [
        "# Mack code\n",
        "forests_HypInputs_Pipeline.fit(df_combined_22_23[forests_HypInputs_features], df_combined_22_23[\"winner\"])\n",
        "\n",
        "forests_HypInputs_Pipeline.predict(df_combined_23_24[forests_HypInputs_features])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOqiyqMTODbw"
      },
      "outputs": [],
      "source": [
        "# Mack code\n",
        "# MACKS BEST MODEL --> RANDOMFORESTCLASSIFIER: INPUT FEATURES --> HYPERPARAMS\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
        "\n",
        "# Define features\n",
        "forests_2_best_features = ['Seed_diff', 'SOS_diff', 'Opp._diff', 'TRB_diff', 'STL_diff', 'BLK_diff', 'PF_diff']\n",
        "\n",
        "# Prepare training data (2022–23)\n",
        "X_train = df_combined_22_23[forests_2_best_features]\n",
        "Y_train = df_combined_22_23[\"winner\"]\n",
        "\n",
        "# Prepare test data (2023–24)\n",
        "X_test = df_combined_23_24[forests_2_best_features]\n",
        "Y_test = df_combined_23_24[\"winner\"]\n",
        "\n",
        "# Define hyperparameters and create pipeline\n",
        "forests_HypInputs_Scaler = Normalizer()\n",
        "forests_HypInputs_max_depth = 5\n",
        "forests_HypInputs_min_samples_split = 5\n",
        "forests_HypInputs_n_estimators = 100\n",
        "\n",
        "forests_HypInputs_Pipeline = make_pipeline(\n",
        "    forests_HypInputs_Scaler,\n",
        "    RandomForestClassifier(\n",
        "        max_depth=forests_HypInputs_max_depth,\n",
        "        min_samples_split=forests_HypInputs_min_samples_split,\n",
        "        n_estimators=forests_HypInputs_n_estimators,\n",
        "        random_state=42\n",
        "    )\n",
        ")\n",
        "\n",
        "# Fit the pipeline on the training data\n",
        "forests_HypInputs_Pipeline.fit(X_train, Y_train)\n",
        "\n",
        "# Predict on the test data\n",
        "Y_pred = forests_HypInputs_Pipeline.predict(X_test)\n",
        "\n",
        "# Evaluate predictions\n",
        "accuracy = accuracy_score(Y_test, Y_pred)\n",
        "f1 = f1_score(Y_test, Y_pred, average=\"macro\")\n",
        "cm = confusion_matrix(Y_test, Y_pred)\n",
        "report = classification_report(Y_test, Y_pred)\n",
        "\n",
        "print(f\"Accuracy on 2023–24 data: {accuracy:.3f}\")\n",
        "print(f\"F1-macro on 2023–24 data: {f1:.3f}\")\n",
        "print(\"Confusion Matrix:\\n\", cm)\n",
        "print(\"\\nClassification Report:\\n\", report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6oeDvyMaSnWU"
      },
      "outputs": [],
      "source": [
        "# Mack code\n",
        "Y_proba = forests_HypInputs_Pipeline.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute precision, recall, and thresholds\n",
        "precision, recall, thresholds = precision_recall_curve(Y_test, Y_proba, pos_label=1)\n",
        "\n",
        "# Plot the precision-recall curve\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(recall, precision, label=\"Precision-Recall Curve\", color='b')\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"Precision-Recall Curve (2023–24 Data)\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5raKsc5lBSGp"
      },
      "outputs": [],
      "source": [
        "# Mack code\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "rf_model = forests_HypInputs_Pipeline.named_steps[\"randomforestclassifier\"]  # if that's the name\n",
        "importances = rf_model.feature_importances_\n",
        "\n",
        "# Use the features from your best model (best_features_forests)\n",
        "feature_names = forests_HypInputs_features  # Changed this line\n",
        "\n",
        "feat_imp_df = pd.DataFrame({\"Feature\": forests_HypInputs_features, \"Importance\": importances})\n",
        "feat_imp_df.sort_values(\"Importance\", ascending=False, inplace=True)\n",
        "\n",
        "plt.barh(feat_imp_df[\"Feature\"], feat_imp_df[\"Importance\"])\n",
        "plt.xlabel(\"Feature Importance\")\n",
        "plt.title(\"RandomForest Coefficients\")\n",
        "plt.gca().invert_yaxis()  # so top feature is at the top\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKTERL4YBnJT"
      },
      "outputs": [],
      "source": [
        "# Mack code\n",
        "# ensemble model of randomforestclassifier\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np\n",
        "\n",
        "# Create two pipelines with different scalers\n",
        "pipeline1 = make_pipeline(MaxAbsScaler(), RandomForestClassifier(max_depth=5, min_samples_split=2, n_estimators=100))\n",
        "\n",
        "pipeline2 = make_pipeline(forests_HypInputs_Scaler, RandomForestClassifier(max_depth=forests_HypInputs_max_depth, min_samples_split=forests_HypInputs_min_samples_split, n_estimators=forests_HypInputs_n_estimators))\n",
        "\n",
        "# Create a VotingClassifier that combines the pipelines (using soft voting)\n",
        "ensemble_model = VotingClassifier(\n",
        "    estimators=[('standard_rf', pipeline1), ('minmax_rf', pipeline2)],\n",
        "    voting='soft'\n",
        ")\n",
        "\n",
        "# Evaluate using cross-validation\n",
        "scores = cross_val_score(ensemble_model, x_train, y_train, cv=10, scoring=\"f1_macro\", n_jobs=-1)\n",
        "print(\"Cross-validated F1-macro scores:\", scores)\n",
        "print(\"Mean F1-macro score:\", np.mean(scores))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_F9VKsBT39fB"
      },
      "outputs": [],
      "source": [
        "# Kevin Code\n",
        "# Base features variable:\n",
        "features = [\"Seed_diff\", \"W-L%_diff\", \"SRS_diff\", \"SOS_diff\", \"Tm._diff\", \"Opp._diff\", \"FG%_diff\", \"3P%_diff\", \"TRB_diff\", \"AST_diff\", \"STL_diff\", \"BLK_diff\", \"TOV_diff\", \"PF_diff\"]\n",
        "\n",
        "\n",
        "# Model libraries needed along with scalers and metrics\n",
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.preprocessing import (MaxAbsScaler, MinMaxScaler, Normalizer, PowerTransformer, QuantileTransformer, RobustScaler, StandardScaler)\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from itertools import combinations\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# List of scalers and distance metrics\n",
        "scalers = [MaxAbsScaler(), MinMaxScaler(), Normalizer(), PowerTransformer(), QuantileTransformer(n_quantiles=60, output_distribution='normal'), RobustScaler(), StandardScaler()]\n",
        "dist_metrics = [\"euclidean\", \"manhattan\", \"chebyshev\", \"minkowski\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxbuF-LzsFg3"
      },
      "outputs": [],
      "source": [
        "# Kevin Code\n",
        "\n",
        "# model 3 -- kneighborsclassifier (Tuning Hyperparameters/Scaler, then input features)\n",
        "\n",
        "# Training data starting with all features first\n",
        "X_train = df_combined_22_23[features]\n",
        "Y_train = df_combined_22_23[\"winner\"]\n",
        "\n",
        "# Variables to store the best hyperparameters/scaler and the f1_macro score of the model with the hyperparameters/scaler\n",
        "best_score = 0\n",
        "best_hypes = []\n",
        "\n",
        "# Going through every combination of scalers and hyperparameters\n",
        "for scaler in scalers:\n",
        "  pipeline = make_pipeline(scaler, KNeighborsClassifier())\n",
        "  grid_cv = GridSearchCV(pipeline,\n",
        "                         param_grid={\"kneighborsclassifier__n_neighbors\": range(1, 30),\"kneighborsclassifier__metric\": dist_metrics},\n",
        "                         scoring=\"f1_macro\", cv=10)\n",
        "  grid_cv.fit(X_train, Y_train)\n",
        "\n",
        "  potential_model = [f\"Scaler: {scaler}\"]\n",
        "  for key, value in (grid_cv.best_params_).items():\n",
        "    potential_model.append(f\"{key}: {value}\")\n",
        "  score = grid_cv.best_score_\n",
        "\n",
        "  if score > best_score:\n",
        "    best_score = score\n",
        "    best_hypes = potential_model\n",
        "\n",
        "print(f\"{best_hypes}: {best_score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PcW04XPbI75x"
      },
      "outputs": [],
      "source": [
        "# Kevin Code\n",
        "\n",
        "# Storing the best hyperparamets/scaler for the model when tuning hyperparameters first\n",
        "best_scaler = MinMaxScaler()\n",
        "best_metric = \"chebyshev\"\n",
        "best_neighbors = 5\n",
        "\n",
        "# Tuning the input features\n",
        "\n",
        "# The pipeline with the best scaler and model with the best hyperparameters\n",
        "best_pipeline = make_pipeline(best_scaler, KNeighborsClassifier(n_neighbors=best_neighbors, metric=best_metric))\n",
        "\n",
        "# Variables to store the best input features to use for the model and the f1_score it produces\n",
        "best_score = 0\n",
        "best_features = []\n",
        "\n",
        "# Going through every combination of input features\n",
        "for i in tqdm(range(1,len(features) + 1)):\n",
        "  for combo in tqdm(combinations(features, i)):\n",
        "    features_combo = list(combo)\n",
        "    score = cross_val_score(best_pipeline,\n",
        "                    X=df_combined_22_23[features_combo], y=df_combined_22_23[\"winner\"],\n",
        "                    scoring=\"f1_macro\", cv=10).mean()\n",
        "    if score > best_score:\n",
        "      best_score = score\n",
        "      best_features = features_combo\n",
        "\n",
        "print(f\"{best_features}: {best_score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGr7Epo09Gns"
      },
      "outputs": [],
      "source": [
        "# Kevin Code\n",
        "\n",
        "# Storing the best hyperparameters/scaler and best features for the KNeighborsClassifier model after tuning hyperparameters/scaler first and then input features\n",
        "kneighborsModel_HypInputs_Scaler = MinMaxScaler()\n",
        "kneighborsModel_HypInputs_metric = \"chebyshev\"\n",
        "kneighborsModel_HypInputs_neighbors = 5\n",
        "kneighborsModel_HypInputs_features = ['SRS_diff', 'SOS_diff', 'Tm._diff', 'Opp._diff', 'TRB_diff', 'AST_diff', 'BLK_diff', 'TOV_diff', 'PF_diff']\n",
        "kneighborsModel_HypInputs_Pipeline = make_pipeline(kneighborsModel_HypInputs_Scaler,\n",
        "                                                   KNeighborsClassifier(n_neighbors=kneighborsModel_HypInputs_neighbors, metric=kneighborsModel_HypInputs_metric))\n",
        "\n",
        "# Storing the f1_macro score for this model\n",
        "kneighborsModel_HypInputs_XTrain = df_combined_22_23[kneighborsModel_HypInputs_features]\n",
        "kneighborsModel_HypInputs_YTrain = df_combined_22_23[\"winner\"]\n",
        "\n",
        "kneighborsModel_HypInputs_Score = cross_val_score(kneighborsModel_HypInputs_Pipeline, kneighborsModel_HypInputs_XTrain, kneighborsModel_HypInputs_YTrain, scoring= \"f1_macro\", cv= 10).mean()\n",
        "kneighborsModel_HypInputs_Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNn8gRWTd1bA"
      },
      "outputs": [],
      "source": [
        "# Kevin Code\n",
        "\n",
        "# model 3 -- kneighborsclassifier (Tuning input features, then Hyperparameters/Scaler)\n",
        "\n",
        "# Making a general starting pipeline to find the best input features for the model first\n",
        "start_pipeline = make_pipeline(StandardScaler(), KNeighborsClassifier(n_neighbors=10))\n",
        "\n",
        "# Variables to store the best input features for the general model and the f1_macro score they produce\n",
        "best_score = 0\n",
        "best_features = []\n",
        "\n",
        "# Going through every combination of input features\n",
        "for i in tqdm(range(1,len(features) + 1)):\n",
        "  for combo in tqdm(combinations(features, i)):\n",
        "    features_combo = list(combo)\n",
        "    score = cross_val_score(start_pipeline,\n",
        "                    X=df_combined_22_23[features_combo], y=df_combined_22_23[\"winner\"],\n",
        "                    scoring=\"f1_macro\", cv=10).mean()\n",
        "    if score > best_score:\n",
        "      best_score = score\n",
        "      best_features = features_combo\n",
        "print(f\"{best_features}: {best_score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bl2gycNEBmiW"
      },
      "outputs": [],
      "source": [
        "# Kevin Code\n",
        "\n",
        "# Varaibles to store the training data according the the best input features found above\n",
        "X_train = df_combined_22_23[best_features]\n",
        "Y_train = df_combined_22_23[\"winner\"]\n",
        "\n",
        "# Variables to store the best hyperparameters/scaler for the model with the best features and the f1_macro score it outputs\n",
        "best_score = 0\n",
        "best_hypes = []\n",
        "\n",
        "# Going through every combination of scalers and hyperparameters\n",
        "for scaler in tqdm(scalers):\n",
        "  pipeline = make_pipeline(scaler, KNeighborsClassifier())\n",
        "\n",
        "  grid_cv = GridSearchCV(pipeline,\n",
        "                         param_grid={\"kneighborsclassifier__n_neighbors\": range(1, 30),\"kneighborsclassifier__metric\": dist_metrics},\n",
        "                         scoring=\"f1_macro\", cv=10)\n",
        "  grid_cv.fit(X_train, Y_train)\n",
        "\n",
        "  potential_model = [f\"Scaler: {scaler}\"]\n",
        "  for key, value in (grid_cv.best_params_).items():\n",
        "    potential_model.append(f\"{key}: {value}\")\n",
        "  score = grid_cv.best_score_\n",
        "\n",
        "  if score > best_score:\n",
        "    best_score = score\n",
        "    best_hypes = potential_model\n",
        "\n",
        "print(f\"{best_hypes}: {best_score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Jm51DkEB1Xx"
      },
      "outputs": [],
      "source": [
        "# Kevin Code\n",
        "\n",
        "# Storing the best hyperparameters/scaler and best features for the KNeighborsClassifier model after tuning input features first and then hyperparameters/scaler\n",
        "kneighborsModel_InputsHyp_Scaler = QuantileTransformer(n_quantiles=60, output_distribution='normal')\n",
        "kneighborsModel_InputsHyp_metric = \"chebyshev\"\n",
        "kneighborsModel_InputsHyp_neighbors = 11\n",
        "kneighborsModel_InputsHyp_features = ['Seed_diff', 'W-L%_diff', 'SRS_diff', 'Tm._diff', 'Opp._diff', 'STL_diff', 'BLK_diff']\n",
        "kneighborsModel_InputsHyp_Pipeline = make_pipeline(kneighborsModel_InputsHyp_Scaler,\n",
        "                                                   KNeighborsClassifier(n_neighbors=kneighborsModel_InputsHyp_neighbors, metric=kneighborsModel_InputsHyp_metric))\n",
        "\n",
        "kneighborsModel_InputsHyp_XTrain = df_combined_22_23[kneighborsModel_InputsHyp_features]\n",
        "kneighborsModel_InputsHyp_YTrain = df_combined_22_23[\"winner\"]\n",
        "\n",
        "kneighborsModel_InputsHyp_Score = cross_val_score(kneighborsModel_InputsHyp_Pipeline, kneighborsModel_InputsHyp_XTrain, kneighborsModel_InputsHyp_YTrain, scoring= \"f1_macro\", cv= 10).mean()\n",
        "kneighborsModel_InputsHyp_Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tuqxiUUr1igV"
      },
      "outputs": [],
      "source": [
        "# Kevin Code\n",
        "\n",
        "# Ensembling the two KNeighborsClassifier models using VotingRegressor to see if a better model is produced\n",
        "\n",
        "# Joining the input features of both models together to use with VotingRegressor\n",
        "ensem_features = list(set(kneighborsModel_HypInputs_features + kneighborsModel_InputsHyp_features))\n",
        "\n",
        "# Getting the training data accoriding to the joined inputs\n",
        "X_train = df_combined_22_23[ensem_features]\n",
        "Y_train = df_combined_22_23[\"winner\"]\n",
        "\n",
        "# Creating the ensemble model and outputing its f1_marco score\n",
        "kneighborsModel_Ensemble = VotingClassifier([(\"kneighborsModel_HypInputs\", kneighborsModel_HypInputs_Pipeline), (\"kneighborsModel_InputsHyp\", kneighborsModel_InputsHyp_Pipeline)])\n",
        "\n",
        "kneighborsModel_Ensemble_Score = cross_val_score(kneighborsModel_Ensemble, X_train, Y_train, scoring= \"f1_macro\", cv= 10).mean()\n",
        "kneighborsModel_Ensemble_Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYK7ei7O6VAq"
      },
      "outputs": [],
      "source": [
        "# Kevin Code\n",
        "\n",
        "# Evaluating the scores for all the KNeighborsClassifier models to see which was the best one\n",
        "print(f\"kneighborsModel_HypInputs_Score : {kneighborsModel_HypInputs_Score}\")\n",
        "print(f\"kneighborsModel_InputsHyp_Score : {kneighborsModel_InputsHyp_Score}\")\n",
        "print(f\"kneighborsModel_Ensemble_Score : {kneighborsModel_Ensemble_Score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POqqCpwx2kcs"
      },
      "outputs": [],
      "source": [
        "# Kevin Code\n",
        "\n",
        "# model 4 -- GaussianNB (Naive Bayes) (Tuning Hyperparameters/Scaler, then input features)\n",
        "# Doing everything I did above for model 3 but for model 4 which is the GaussianNB model\n",
        "\n",
        "X_train = df_combined_22_23[features]\n",
        "Y_train = df_combined_22_23[\"winner\"]\n",
        "\n",
        "best_hypes = []\n",
        "best_score = 0\n",
        "\n",
        "for scaler in tqdm(scalers):\n",
        "  pipeline = make_pipeline(scaler, GaussianNB())\n",
        "\n",
        "  grid_cv = GridSearchCV(pipeline,\n",
        "                         param_grid={\"gaussiannb__var_smoothing\": np.logspace(-12, 0, 1000)},\n",
        "                         scoring=\"f1_macro\", cv=10)\n",
        "  grid_cv.fit(X_train, Y_train)\n",
        "\n",
        "  potential_model = [f\"Scaler: {scaler}\"]\n",
        "  for key, value in (grid_cv.best_params_).items():\n",
        "    potential_model.append(f\"{key}: {value}\")\n",
        "  score = grid_cv.best_score_\n",
        "\n",
        "  if score > best_score:\n",
        "    best_score = score\n",
        "    best_hypes = potential_model\n",
        "\n",
        "print(f\"{best_hypes}: {best_score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ua5GEkHVpBxp"
      },
      "outputs": [],
      "source": [
        "# Kevin Code\n",
        "\n",
        "best_scaler = QuantileTransformer(n_quantiles=60, output_distribution='normal')\n",
        "best_varSmoothing = 1e-12\n",
        "\n",
        "best_pipeline = make_pipeline(best_scaler, GaussianNB(var_smoothing= best_varSmoothing))\n",
        "\n",
        "best_score = 0\n",
        "best_features = []\n",
        "for i in tqdm(range(1,len(features) + 1)):\n",
        "  for combo in tqdm(combinations(features, i)):\n",
        "    features_combo = list(combo)\n",
        "    score = cross_val_score(best_pipeline,\n",
        "                    X=df_combined_22_23[features_combo], y=df_combined_22_23[\"winner\"],\n",
        "                    scoring=\"f1_macro\", cv=10).mean()\n",
        "    if score > best_score:\n",
        "      best_score = score\n",
        "      best_features = features_combo\n",
        "\n",
        "print(f\"{best_features}: {best_score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btoG_RjOsQVh"
      },
      "outputs": [],
      "source": [
        "# Kevin Code\n",
        "\n",
        "gaussianModel_HypInputs_Scaler = QuantileTransformer(n_quantiles=60, output_distribution='normal')\n",
        "gaussianModel_HypInputs_smoothing = 1e-12\n",
        "gaussianModel_HypInputs_features = ['SOS_diff', 'Tm._diff', 'Opp._diff', '3P%_diff', 'TRB_diff', 'STL_diff', 'BLK_diff']\n",
        "gaussianModel_HypInputs_Pipeline = make_pipeline(gaussianModel_HypInputs_Scaler,\n",
        "                                                  GaussianNB(var_smoothing=gaussianModel_HypInputs_smoothing))\n",
        "\n",
        "gaussianModel_HypInputs_XTrain = df_combined_22_23[gaussianModel_HypInputs_features]\n",
        "gaussianModel_HypInputs_YTrain = df_combined_22_23[\"winner\"]\n",
        "\n",
        "gaussianModel_HypInputs_Score = cross_val_score(gaussianModel_HypInputs_Pipeline, gaussianModel_HypInputs_XTrain, gaussianModel_HypInputs_YTrain, scoring= \"f1_macro\", cv= 10).mean()\n",
        "gaussianModel_HypInputs_Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_bB5uA6ej7U"
      },
      "outputs": [],
      "source": [
        "# Kevin Code\n",
        "\n",
        "# model 4 -- GaussianNB (NavieBayes) (Tuning input features, then Hyperparameters/Scaler)\n",
        "\n",
        "start_pipeline = make_pipeline(StandardScaler(), GaussianNB())\n",
        "\n",
        "best_score = 0\n",
        "best_features = []\n",
        "for i in tqdm(range(1,len(features) + 1)):\n",
        "  for combo in tqdm(combinations(features, i)):\n",
        "    features_combo = list(combo)\n",
        "    score = cross_val_score(start_pipeline,\n",
        "                    X=df_combined_22_23[features_combo], y=df_combined_22_23[\"winner\"],\n",
        "                    scoring=\"f1_macro\", cv=10).mean()\n",
        "    if score > best_score:\n",
        "      best_score = score\n",
        "      best_features = features_combo\n",
        "print(f\"{best_features}: {best_score}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AaocBreZr9jW"
      },
      "outputs": [],
      "source": [
        "# Kevin Code\n",
        "\n",
        "X_train = df_combined_22_23[best_features]\n",
        "Y_train = df_combined_22_23[\"winner\"]\n",
        "\n",
        "best_hypes = []\n",
        "best_score = 0\n",
        "\n",
        "for scaler in tqdm(scalers):\n",
        "  pipeline = make_pipeline(scaler, GaussianNB())\n",
        "\n",
        "  grid_cv = GridSearchCV(pipeline,\n",
        "                         param_grid={\"gaussiannb__var_smoothing\": np.logspace(-12, 0, 1000)},\n",
        "                         scoring=\"f1_macro\", cv=10)\n",
        "  grid_cv.fit(X_train, Y_train)\n",
        "\n",
        "  potential_model = [f\"Scaler: {scaler}\"]\n",
        "  for key, value in (grid_cv.best_params_).items():\n",
        "    potential_model.append(f\"{key}: {value}\")\n",
        "  score = grid_cv.best_score_\n",
        "\n",
        "  if score > best_score:\n",
        "    best_score = score\n",
        "    best_hypes = potential_model\n",
        "\n",
        "print(f\"{best_hypes}: {best_score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcwYHBWhsPLX"
      },
      "outputs": [],
      "source": [
        "# Kevin Code\n",
        "\n",
        "gaussianModel_InputsHyp_Scaler = MaxAbsScaler()\n",
        "gaussianModel_InputsHyp_smoothing = 1e-12\n",
        "gaussianModel_InputsHyp_features = ['SRS_diff', 'Tm._diff', 'PF_diff']\n",
        "gaussianModel_InputsHyp_Pipeline = make_pipeline(gaussianModel_InputsHyp_Scaler,\n",
        "                                                  GaussianNB(var_smoothing=gaussianModel_InputsHyp_smoothing))\n",
        "\n",
        "gaussianModel_InputsHyp_XTrain = df_combined_22_23[gaussianModel_InputsHyp_features]\n",
        "gaussianModel_InputsHyp_YTrain = df_combined_22_23[\"winner\"]\n",
        "\n",
        "gaussianModel_InputsHyp_Score = cross_val_score(gaussianModel_InputsHyp_Pipeline, gaussianModel_InputsHyp_XTrain, gaussianModel_InputsHyp_YTrain, scoring= \"f1_macro\", cv= 10).mean()\n",
        "gaussianModel_InputsHyp_Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbwDY1RU4UGP"
      },
      "outputs": [],
      "source": [
        "# Kevin Code\n",
        "\n",
        "# Ensembling the two GaussianNB models using VotingRegressor to see if a better model is produced\n",
        "\n",
        "ensem_features = list(set(gaussianModel_HypInputs_features + gaussianModel_InputsHyp_features))\n",
        "\n",
        "X_train = df_combined_22_23[ensem_features]\n",
        "Y_train = df_combined_22_23[\"winner\"]\n",
        "\n",
        "gaussianModel_Ensemble = VotingClassifier([(\"gaussianModel_HypInputs\", gaussianModel_HypInputs_Pipeline), (\"gaussianModel_InputsHyp\", gaussianModel_InputsHyp_Pipeline)])\n",
        "\n",
        "gaussianModel_Ensemble_Score = cross_val_score(gaussianModel_Ensemble, X_train, Y_train, scoring= \"f1_macro\", cv= 10).mean()\n",
        "gaussianModel_Ensemble_Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKfV2B9p62og"
      },
      "outputs": [],
      "source": [
        "# Kevin Code\n",
        "\n",
        "print(f\"gaussianModel_HypInputs_Score : {gaussianModel_HypInputs_Score}\")\n",
        "print(f\"gaussianModel_InputsHyp_Score : {gaussianModel_InputsHyp_Score}\")\n",
        "print(f\"gaussianModel_Ensemble_score : {gaussianModel_Ensemble_Score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47izaUGi8vWT"
      },
      "outputs": [],
      "source": [
        "# Kevin Code\n",
        "\n",
        "# Getting the pipelines and features data for the best KNeighborsClassifier model and GaussianNB model according to the evalulations above\n",
        "\n",
        "best_kneighborsModel_Pipeline = kneighborsModel_InputsHyp_Pipeline\n",
        "best_gaussianModel_Pipeline = gaussianModel_HypInputs_Pipeline\n",
        "\n",
        "best_kneighborsModel_XTrain = kneighborsModel_InputsHyp_XTrain\n",
        "best_gaussianModel_XTrain = gaussianModel_HypInputs_XTrain\n",
        "\n",
        "# Making a global variable for all the known outcomes of the training data\n",
        "all_YTrain = df_combined_22_23[\"winner\"]\n",
        "\n",
        "# Storing the test data for the features of each model\n",
        "best_kneighborsModel_XTest = df_combined_23_24[kneighborsModel_InputsHyp_features]\n",
        "best_gaussianModel_XTest = df_combined_23_24[gaussianModel_HypInputs_features]\n",
        "\n",
        "# Making a global variable for all the known outcomes of the test data\n",
        "all_YTest = df_combined_23_24[\"winner\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qzcJkZpAZTK"
      },
      "outputs": [],
      "source": [
        "# Kevin Code\n",
        "\n",
        "# Graphing the importance of each input for the best KNeighborsClassifier model\n",
        "\n",
        "# Using permutation_importance learned about online\n",
        "\n",
        "from sklearn.inspection import permutation_importance\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Fitting the training data to the model\n",
        "best_kneighborsModel_Pipeline.fit(best_kneighborsModel_XTrain, all_YTrain)\n",
        "\n",
        "# Storing the information from permutation_importance function\n",
        "perm_importance = permutation_importance(best_kneighborsModel_Pipeline, best_kneighborsModel_XTrain, all_YTrain, scoring= \"f1_macro\", n_repeats= 20, random_state= 0)\n",
        "\n",
        "# Storing the importance means for each input which represents the accuracy drop when a feature is shuffled.\n",
        "# So the higher the value, the more important a feature is to the model.\n",
        "best_kneighborsModel_InputsImportance = perm_importance.importances_mean\n",
        "\n",
        "# Plotting the input importance\n",
        "\n",
        "# Sorting the input importance values in descending order so that when graph,\n",
        "# longest bar is at the top\n",
        "best_InputNames = best_kneighborsModel_XTrain.columns\n",
        "sorted_iputs = np.argsort(best_kneighborsModel_InputsImportance)\n",
        "sorted_importances = best_kneighborsModel_InputsImportance[sorted_iputs]\n",
        "sorted_InputNames = best_InputNames[sorted_iputs]\n",
        "\n",
        "# Plotting horizontal bar graph of inputs importance\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.barh(sorted_InputNames, sorted_importances)\n",
        "plt.xlabel(\"Input Importance Score\")\n",
        "plt.ylabel(\"Input\")\n",
        "plt.title(\"Permutation Input Importance for best KNeighborsClassifier Model\")\n",
        "plt.gca()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aVDPQTY_LBe9"
      },
      "outputs": [],
      "source": [
        "# Kevin Code\n",
        "\n",
        "# Graphing the importance of each input for the best GaussianNB model\n",
        "# Just as done above with KNeighborsClassifier\n",
        "\n",
        "best_gaussianModel_Pipeline.fit(best_gaussianModel_XTrain, all_YTrain)\n",
        "\n",
        "perm_importance = permutation_importance(best_gaussianModel_Pipeline, best_gaussianModel_XTrain, all_YTrain, scoring= \"f1_macro\", n_repeats= 20, random_state= 0)\n",
        "\n",
        "best_gaussianModel_InputsImportance = perm_importance.importances_mean\n",
        "\n",
        "\n",
        "best_InputNames = best_gaussianModel_XTrain.columns\n",
        "sorted_iputs = np.argsort(best_gaussianModel_InputsImportance)\n",
        "sorted_importances = best_gaussianModel_InputsImportance[sorted_iputs]\n",
        "sorted_InputNames = best_InputNames[sorted_iputs]\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.barh(sorted_InputNames, sorted_importances)\n",
        "plt.xlabel(\"Input Importance Score\")\n",
        "plt.ylabel(\"Input\")\n",
        "plt.title(\"Permutation Input Importance for best GaussianNB Model\")\n",
        "plt.gca()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSlKPqM66jFA"
      },
      "outputs": [],
      "source": [
        "# Kevin Code\n",
        "\n",
        "# Getting the f1_macro score for the best KNeighborsClassifier model and GaussianNB model on the test data to see which is better\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "best_kneighborsModel_Pipeline.fit(best_kneighborsModel_XTrain, all_YTrain)\n",
        "best_kneighborsModel_YTest_predictions = best_kneighborsModel_Pipeline.predict(best_kneighborsModel_XTest)\n",
        "\n",
        "best_kneighborsModel_YTest_f1macro = f1_score(all_YTest, best_kneighborsModel_YTest_predictions, average=\"macro\")\n",
        "\n",
        "best_gaussianModel_Pipeline.fit(best_gaussianModel_XTrain, all_YTrain)\n",
        "best_gaussianModel_YTest_predictions = best_gaussianModel_Pipeline.predict(best_gaussianModel_XTest)\n",
        "\n",
        "best_gaussianModel_YTest_f1macro = f1_score(all_YTest, best_gaussianModel_YTest_predictions, average=\"macro\")\n",
        "\n",
        "print(f\"Best KNeighborsClassifier f1_score: {best_kneighborsModel_YTest_f1macro}\")\n",
        "print(f\"Best GaussianNB f1_score: {best_gaussianModel_YTest_f1macro}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVt1_8BXihb3"
      },
      "outputs": [],
      "source": [
        "# Kevin Code\n",
        "\n",
        "# Checking the estimated test error if using the 23-24 data as training data\n",
        "\n",
        "best_kneighborsModel_YTest_f1macro = cross_val_score(best_kneighborsModel_Pipeline, best_kneighborsModel_XTest, all_YTest, scoring=\"f1_macro\", cv=10).mean()\n",
        "best_gaussianModel_YTest_f1macro = cross_val_score(best_gaussianModel_Pipeline, best_gaussianModel_XTest, all_YTest, scoring=\"f1_macro\", cv=10).mean()\n",
        "\n",
        "print(f\"Best KNeighborsClassifier f1_score: {best_kneighborsModel_YTest_f1macro}\")\n",
        "print(f\"Best GaussianNB f1_score: {best_gaussianModel_YTest_f1macro}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3-RWXua76ME"
      },
      "outputs": [],
      "source": [
        "# Kevin Code\n",
        "\n",
        "# Graphing the precision-recall curve for the best KNeighborsClassifier model on the test data\n",
        "\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "# Getting the probabilities produced by the model on the training data itself\n",
        "best_kneighborsModel_Ytest_probs_ = best_kneighborsModel_Pipeline.predict_proba(best_kneighborsModel_XTest)\n",
        "\n",
        "# Storing the precision and recall values for different thresholds using the precision_recall_curve function\n",
        "# for the models accuracy on predicting the correct winner\n",
        "kneighborsModel_precision, kneighborsModel_recall, kneighborsModel_thresholds = precision_recall_curve(\n",
        "    all_YTest, best_kneighborsModel_Ytest_probs_[:, 1], pos_label=1)\n",
        "\n",
        "# Plotting the precision-recall curve\n",
        "pd.DataFrame({\n",
        "    \"precision\": kneighborsModel_precision,\n",
        "    \"recall\": kneighborsModel_recall\n",
        "}).plot.line(x=\"precision\", y=\"recall\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gyUyj-zvLghq"
      },
      "outputs": [],
      "source": [
        "# Kevin Code\n",
        "\n",
        "# Graphing the precision-recall curve for the best GaussianNB model as done above for the KNeighborsClassifier model\n",
        "\n",
        "best_gaussianModel_Ytest_probs_ = best_gaussianModel_Pipeline.predict_proba(best_gaussianModel_XTest)\n",
        "\n",
        "gaussianModel_precision, gaussianModel_recall, gaussianModel_thresholds = precision_recall_curve(\n",
        "    all_YTest, best_gaussianModel_Ytest_probs_[:, 1], pos_label=1)\n",
        "\n",
        "pd.DataFrame({\n",
        "    \"precision\": gaussianModel_precision,\n",
        "    \"recall\": gaussianModel_recall\n",
        "}).plot.line(x=\"precision\", y=\"recall\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83zIEPhwYoVi"
      },
      "outputs": [],
      "source": [
        "# Mack Code (Modified by Kevin)\n",
        "# Checking the estimated test error if using the 23-24 data as training data for Mack's models\n",
        "\n",
        "\n",
        "best_features_logistic1 = [\"SOS_diff\", \"Tm._diff\", \"Opp._diff\", \"FG%_diff\", \"3P%_diff\", \"TRB_diff\"]\n",
        "\n",
        "best_pipeline_logistic1 = make_pipeline(\n",
        "    RobustScaler(),\n",
        "    LogisticRegression(C=0.01, solver=\"lbfgs\", penalty=\"l2\", random_state=42)\n",
        ")\n",
        "\n",
        "\n",
        "best_features_logistic2 = ['SRS_diff', 'Opp._diff', 'PF_diff']\n",
        "\n",
        "best_pipeline_logistic2 = make_pipeline(RobustScaler(), LogisticRegression(C=0.01, penalty=\"l2\", solver=\"lbfgs\"))\n",
        "\n",
        "logistic1_f1macro = cross_val_score(best_pipeline_logistic1, df_combined_23_24[best_features_logistic1], all_YTest, scoring=\"f1_macro\", cv=10).mean()\n",
        "logistic2_f1macro = cross_val_score(best_pipeline_logistic2, df_combined_23_24[best_features_logistic2], all_YTest, scoring=\"f1_macro\", cv=10).mean()\n",
        "\n",
        "print(f\"Logistic 1 f1_score: {logistic1_f1macro}\")\n",
        "print(f\"Logistic 2 f1_score: {logistic2_f1macro}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67ktFSeCfu05"
      },
      "outputs": [],
      "source": [
        "# Kevin code\n",
        "# Testing an ensemble of Mack's models\n",
        "\n",
        "pipeline1 = best_pipeline_logistic1\n",
        "pipeline2 = best_pipeline_logistic2\n",
        "\n",
        "ensemble_model = VotingClassifier(\n",
        "    estimators=[('best_pipeline_logistic1', pipeline1), ('best_pipeline_logistic2', pipeline2)],\n",
        "    voting='soft'\n",
        ")\n",
        "\n",
        "logistic_ensemble =cross_val_score(ensemble_model, df_combined_23_24[list(set(best_features_logistic1 + best_features_logistic2))],\n",
        "                all_YTest, scoring=\"f1_macro\", cv=10).mean()\n",
        "print(f\"Forest Ensemble f1_score: {logistic_ensemble}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvGhK7O4aaUF"
      },
      "outputs": [],
      "source": [
        "# Mack Code (Modified by Kevin)\n",
        "# Checking the estimated test error if using the 23-24 data as training data for Mack's models\n",
        "\n",
        "best_features_forests1 = ['SRS_diff', 'SOS_diff', 'Tm._diff', 'Opp._diff']\n",
        "best_pipeline_forests1 = make_pipeline(MaxAbsScaler(), RandomForestClassifier(max_depth=5, min_samples_split=2, n_estimators=100))\n",
        "\n",
        "best_features_forests2 = ['Seed_diff', 'SOS_diff', 'Opp._diff', 'TRB_diff', 'STL_diff', 'BLK_diff', 'PF_diff']\n",
        "best_pipeline_forests2 = make_pipeline(Normalizer(), RandomForestClassifier(max_depth=5, min_samples_split=5,\n",
        "                                                                            n_estimators=100, random_state=42))\n",
        "\n",
        "forest1_f1macro = cross_val_score(best_pipeline_forests1, df_combined_23_24[best_features_forests1], all_YTest, scoring=\"f1_macro\", cv=10).mean()\n",
        "forest2_f1macro = cross_val_score(best_pipeline_forests2, df_combined_23_24[best_features_forests2], all_YTest, scoring=\"f1_macro\", cv=10).mean()\n",
        "\n",
        "print(f\"Forest 1 f1_score: {forest1_f1macro}\")\n",
        "print(f\"Forest 2 f1_score: {forest2_f1macro}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6zqnXhfcTmA"
      },
      "outputs": [],
      "source": [
        "# Kevin code\n",
        "# Testing an ensemble of Mack's models\n",
        "\n",
        "pipeline1 = best_pipeline_forests1\n",
        "pipeline2 = best_pipeline_forests2\n",
        "\n",
        "ensemble_model = VotingClassifier(\n",
        "    estimators=[('best_pipeline_forests1', pipeline1), ('best_pipeline_forests2', pipeline2)],\n",
        "    voting='soft'\n",
        ")\n",
        "\n",
        "forest_ensemble =cross_val_score(ensemble_model, df_combined_23_24[list(set(best_features_forests1 + best_features_forests2))],\n",
        "                all_YTest, scoring=\"f1_macro\", cv=10).mean()\n",
        "print(f\"Forest Ensemble f1_score: {forest_ensemble}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}